{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1b49197-e67a-48fa-b110-0cce10ed6830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c8230b-f6ef-44a9-ad97-2a24687324d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0) Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe39e03-40ad-4f30-ad58-fd88ded4710e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks: instalar Faker (persistente no cluster enquanto ativo)\n",
    "# Se seu cluster já tem Faker, pode ignorar esta célula.\n",
    "%pip install Faker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff4e260-1f37-4ce4-9bea-40792f0fcfcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Parâmetros e helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd11b72-1073-4f59-91f7-2c07d494fc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import random\n",
    "import string\n",
    "\n",
    "# ===== Parâmetros =====\n",
    "CATALOG = \"data_modelling\"            # exemplo: \"workshop_catalog\" ou None se não usar Unity Catalog\n",
    "SCHEMA  = \"bronze\"                    # esquema/database onde serão criadas as tabelas\n",
    "SEED    = 42\n",
    "\n",
    "N_CUSTOMERS   = 1000\n",
    "N_PRODUCTS    = 500\n",
    "N_ORDERS      = 5000\n",
    "N_ORDER_ITEMS = 12000\n",
    "\n",
    "# Percentuais de \"problemas\"\n",
    "P_DUP_ORDERS                = 0.025   # ~2.5% order_id duplicados\n",
    "P_NULL_CUSTOMER_IN_ORDERS   = 0.05    # ~5% customer_id nulos\n",
    "P_STATUS_CASE_VARIATION     = 0.25\n",
    "P_STRING_DATE_IN_ORDERS     = 0.50    # metade como string, metade como date coerente\n",
    "P_STRING_NUMERIC_IN_FIELDS  = 0.15    # % de numéricos como string\n",
    "\n",
    "P_DUP_PRODUCT_ID            = 0.03\n",
    "P_INCONSISTENT_IS_ACTIVE    = 0.40\n",
    "P_NULL_BRAND_SUBCATEGORY    = 0.10\n",
    "\n",
    "P_CUSTOMER_INCONSISTENCY    = 0.20    # estados \"SP\", \"sp\", \"São Paulo\"\n",
    "P_CUSTOMER_DUP_DIFF_UPDATE  = 0.10    # duplicar customer_id com last_update_date diferente\n",
    "P_EMPTY_FIELDS_CUSTOMER     = 0.05\n",
    "\n",
    "P_DUP_ORDERITEM_SAME_OP     = 0.05    # duplicar (order_id, product_id) com updated_at diferente\n",
    "P_NULLS_DISCOUNT_PROMO      = 0.15\n",
    "\n",
    "random.seed(SEED)\n",
    "fake = Faker(\"pt_BR\")\n",
    "Faker.seed(SEED)\n",
    "\n",
    "# ===== Nome totalmente qualificado de tabela =====\n",
    "def fqtn(table):\n",
    "    if CATALOG:\n",
    "        return f\"`{CATALOG}`.`{SCHEMA}`.`{table}`\"\n",
    "    else:\n",
    "        return f\"`{SCHEMA}`.`{table}`\"\n",
    "\n",
    "# ===== Criar schema/database =====\n",
    "if CATALOG:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{CATALOG}`\")\n",
    "    spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`\")\n",
    "else:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{SCHEMA}`\")\n",
    "\n",
    "# ===== Utilidades =====\n",
    "STATUSES = [\"delivered\",\"shipped\",\"processing\",\"cancelled\",\"returned\"]\n",
    "\n",
    "def random_status_inconsistent():\n",
    "    s = random.choice(STATUSES)\n",
    "    if random.random() < P_STATUS_CASE_VARIATION:\n",
    "        # variações de capitalização\n",
    "        choices = [s.upper(), s.capitalize(), s.lower()]\n",
    "        s = random.choice(choices)\n",
    "    return s\n",
    "\n",
    "def random_date_between(days_back=365):\n",
    "    base = datetime.utcnow()\n",
    "    delta = timedelta(days=random.randint(0, days_back), seconds=random.randint(0, 86399))\n",
    "    d = base - delta\n",
    "    return d\n",
    "\n",
    "def random_date_mixed_formats(dt):\n",
    "    # retorna string em formatos variados (\"/\", \"-\", com/sem tempo)\n",
    "    # ex.: \"2025-10-25\", \"2025/10/25 14:33:20\", \"25/10/2025\", etc.\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y %H:%M:%S\",\n",
    "    ]\n",
    "    return dt.strftime(random.choice(formats))\n",
    "\n",
    "def maybe_stringify_number(x):\n",
    "    # converte numérico para string em parte dos casos\n",
    "    if random.random() < P_STRING_NUMERIC_IN_FIELDS:\n",
    "        return f\"{x}\"\n",
    "    return x\n",
    "\n",
    "def maybe_null(val, p=0.1):\n",
    "    return None if random.random() < p else val\n",
    "\n",
    "def dirty_state(uf):\n",
    "    # introduz inconsistências: \"SP\", \"sp\", \"São Paulo\"\n",
    "    if random.random() < P_CUSTOMER_INCONSISTENCY:\n",
    "        variants = [uf, uf.lower(), \"São Paulo\" if uf.upper()==\"SP\" else uf]\n",
    "        return random.choice(variants)\n",
    "    return uf\n",
    "\n",
    "def random_bool_inconsistent():\n",
    "    # \"true\", \"1\", \"yes\", True, False...\n",
    "    opts = [\"true\",\"1\",\"yes\",\"false\",\"0\",\"no\", True, False]\n",
    "    if random.random() < P_INCONSISTENT_IS_ACTIVE:\n",
    "        return random.choice(opts)\n",
    "    return True\n",
    "\n",
    "def alnum(n=8):\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e99e9ec-77a1-4e62-8c1f-b9d4c6e61d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2) Geração da Bronze – products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326fa60c-34e6-4d02-8e25-29adf185dee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema todo STRING para simular fonte raw heterogênea\n",
    "prod_schema = T.StructType([\n",
    "    T.StructField(\"product_id\",    T.StringType(), True),\n",
    "    T.StructField(\"product_name\",  T.StringType(), True),\n",
    "    T.StructField(\"category\",      T.StringType(), True),\n",
    "    T.StructField(\"subcategory\",   T.StringType(), True),\n",
    "    T.StructField(\"brand\",         T.StringType(), True),\n",
    "    T.StructField(\"cost_price\",    T.StringType(), True),\n",
    "    T.StructField(\"list_price\",    T.StringType(), True),\n",
    "    T.StructField(\"is_active\",     T.StringType(), True),  # inconsistente\n",
    "    T.StructField(\"last_update\",   T.StringType(), True),\n",
    "])\n",
    "\n",
    "rows = []\n",
    "categories = [(\"Eletrônicos\",[\"Smartphones\",\"Notebooks\",\"TVs\",\"Acessórios\"]),\n",
    "              (\"Casa\",[\"Cozinha\",\"Cama/Mesa/Banho\",\"Decoração\"]),\n",
    "              (\"Vestuário\",[\"Camisetas\",\"Calçados\",\"Acessórios\"])]\n",
    "\n",
    "for i in range(N_PRODUCTS):\n",
    "    cat, subs = random.choice(categories)\n",
    "    sub = random.choice(subs)\n",
    "    pid = f\"P{100000+i}\"\n",
    "    name = f\"{random.choice(['Alpha','Pro','Max','Lite','Neo'])} {alnum(4)}\"\n",
    "    brand = None if random.random() < P_NULL_BRAND_SUBCATEGORY else random.choice([\"Acme\",\"Globex\",\"Initech\",\"Umbrella\"])\n",
    "    subc  = None if random.random() < P_NULL_BRAND_SUBCATEGORY else sub\n",
    "\n",
    "    cost  = round(random.uniform(10, 2000), 2)\n",
    "    price = round(cost*random.uniform(1.1, 2.5), 2)\n",
    "    row = Row(\n",
    "        product_id   = pid,\n",
    "        product_name = name,\n",
    "        category     = cat,\n",
    "        subcategory  = subc,\n",
    "        brand        = brand,\n",
    "        cost_price   = str(cost) if random.random()<P_STRING_NUMERIC_IN_FIELDS else f\"{cost}\",\n",
    "        list_price   = str(price) if random.random()<P_STRING_NUMERIC_IN_FIELDS else f\"{price}\",\n",
    "        is_active    = str(random_bool_inconsistent()),\n",
    "        last_update  = random_date_mixed_formats(random_date_between(400))\n",
    "    )\n",
    "    rows.append(row)\n",
    "\n",
    "# Duplicatas intencionais de product_id\n",
    "dup_count = int(N_PRODUCTS * P_DUP_PRODUCT_ID)\n",
    "for _ in range(dup_count):\n",
    "    base = random.choice(rows).asDict()\n",
    "    base[\"brand\"] = base[\"brand\"] if random.random()>0.5 else None\n",
    "    base[\"last_update\"] = random_date_mixed_formats(random_date_between(400))\n",
    "    rows.append(Row(**base))\n",
    "\n",
    "df_products = spark.createDataFrame(rows, schema=prod_schema)\n",
    "df_products.write.mode(\"overwrite\").format(\"delta\").saveAsTable(fqtn(\"products\"))\n",
    "display(df_products.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16379bbc-f1b7-41c5-a425-f837c7e907c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3) Geração da Bronze – customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bc1c51-7cc9-4416-b90a-360c3cb95ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\",       T.StringType(), True),\n",
    "    T.StructField(\"customer_name\",     T.StringType(), True),\n",
    "    T.StructField(\"email\",             T.StringType(), True),\n",
    "    T.StructField(\"city\",              T.StringType(), True),\n",
    "    T.StructField(\"state\",             T.StringType(), True),\n",
    "    T.StructField(\"last_update_date\",  T.StringType(), True),  # às vezes string\n",
    "])\n",
    "\n",
    "ufs = [\"SP\",\"RJ\",\"MG\",\"PR\",\"RS\",\"SC\",\"BA\",\"PE\",\"CE\",\"DF\"]\n",
    "base_rows = []\n",
    "for i in range(N_CUSTOMERS):\n",
    "    cid = f\"C{100000+i}\"\n",
    "    nome = fake.name()\n",
    "    email = f\"{nome.lower().replace(' ','')}@{random.choice(['mail.com','corp.com','example.org'])}\"\n",
    "    city  = fake.city()\n",
    "    uf    = dirty_state(random.choice(ufs))\n",
    "\n",
    "    # às vezes campos vazios\n",
    "    if random.random()<P_EMPTY_FIELDS_CUSTOMER:\n",
    "        if random.random()<0.5: city = \"\"\n",
    "        else: email = \"\"\n",
    "\n",
    "    row = Row(\n",
    "        customer_id      = cid,\n",
    "        customer_name    = nome,\n",
    "        email            = email,\n",
    "        city             = city,\n",
    "        state            = uf,\n",
    "        last_update_date = random_date_mixed_formats(random_date_between(500)) \\\n",
    "                           if random.random()<0.7 else str(random_date_between(500))\n",
    "    )\n",
    "    base_rows.append(row)\n",
    "\n",
    "# duplicar alguns customer_id com last_update_date diferente\n",
    "extra = []\n",
    "for _ in range(int(N_CUSTOMERS * P_CUSTOMER_DUP_DIFF_UPDATE)):\n",
    "    r = random.choice(base_rows).asDict()\n",
    "    r[\"last_update_date\"] = random_date_mixed_formats(random_date_between(200))\n",
    "    # opcionalmente muda cidade/estado simulando mudança real\n",
    "    if random.random()<0.6:\n",
    "        r[\"city\"]  = fake.city()\n",
    "        r[\"state\"] = dirty_state(random.choice(ufs))\n",
    "    extra.append(Row(**r))\n",
    "\n",
    "df_customers = spark.createDataFrame(base_rows + extra, schema=cust_schema)\n",
    "df_customers.write.mode(\"overwrite\").format(\"delta\").saveAsTable(fqtn(\"customers\"))\n",
    "display(df_customers.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcda5f58-f024-48e9-9012-4cb9da6ea8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4) Geração da Bronze – orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8073c5b-d852-42b3-93ae-bde1b05554ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\",     T.StringType(), True),\n",
    "    T.StructField(\"customer_id\",  T.StringType(), True),\n",
    "    T.StructField(\"order_date\",   T.StringType(), True),  # datas como STRING (formatos variados)\n",
    "    T.StructField(\"order_status\", T.StringType(), True),\n",
    "    T.StructField(\"total_amount\", T.StringType(), True),  # valores às vezes string\n",
    "])\n",
    "\n",
    "customer_ids = [r[\"customer_id\"] for r in df_customers.select(\"customer_id\").distinct().collect()]\n",
    "\n",
    "rows = []\n",
    "for i in range(N_ORDERS):\n",
    "    oid = f\"O{100000+i}\"\n",
    "    cust = None if random.random()<P_NULL_CUSTOMER_IN_ORDERS else random.choice(customer_ids)\n",
    "    dt   = random_date_between(365)\n",
    "    order_date = random_date_mixed_formats(dt) if random.random()<P_STRING_DATE_IN_ORDERS else dt.strftime(\"%Y-%m-%d\")\n",
    "    status = random_status_inconsistent()\n",
    "    total = round(random.uniform(20, 5000), 2)\n",
    "    total = str(total) if random.random()<P_STRING_NUMERIC_IN_FIELDS else f\"{total}\"\n",
    "\n",
    "    rows.append(Row(\n",
    "        order_id     = oid,\n",
    "        customer_id  = cust,\n",
    "        order_date   = order_date,\n",
    "        order_status = status,\n",
    "        total_amount = total\n",
    "    ))\n",
    "\n",
    "# duplicatas ~2.5%\n",
    "for _ in range(int(N_ORDERS * P_DUP_ORDERS)):\n",
    "    r = random.choice(rows)\n",
    "    # dup com pequenas variações: status/campos\n",
    "    d = r.asDict()\n",
    "    d[\"order_status\"] = random_status_inconsistent()\n",
    "    rows.append(Row(**d))\n",
    "\n",
    "df_orders = spark.createDataFrame(rows, schema=orders_schema)\n",
    "df_orders.write.mode(\"overwrite\").format(\"delta\").saveAsTable(fqtn(\"orders\"))\n",
    "display(df_orders.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea64e14-f6fb-4df4-a3b9-96e34e91999c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5) Geração da Bronze – order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb79bac-02ba-4d4d-a15f-027fc60474b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "items_schema = T.StructType([\n",
    "    T.StructField(\"order_item_id\",  T.StringType(), True),\n",
    "    T.StructField(\"order_id\",       T.StringType(), True),\n",
    "    T.StructField(\"product_id\",     T.StringType(), True),\n",
    "    T.StructField(\"quantity\",       T.StringType(), True),       # numérico como string às vezes\n",
    "    T.StructField(\"unit_price\",     T.StringType(), True),\n",
    "    T.StructField(\"discount_amount\",T.StringType(), True),\n",
    "    T.StructField(\"promotion_id\",   T.StringType(), True),\n",
    "    T.StructField(\"updated_at\",     T.StringType(), True),\n",
    "])\n",
    "\n",
    "order_ids   = [r[\"order_id\"] for r in df_orders.select(\"order_id\").distinct().collect()]\n",
    "product_ids = [r[\"product_id\"] for r in df_products.select(\"product_id\").distinct().collect()]\n",
    "\n",
    "rows = []\n",
    "for i in range(N_ORDER_ITEMS):\n",
    "    oid = random.choice(order_ids)\n",
    "    pid = random.choice(product_ids)\n",
    "    qty = random.randint(1, 5)\n",
    "    unit = round(random.uniform(5, 1500), 2)\n",
    "    disc = round(random.uniform(0, unit*0.3), 2) if random.random()>0.3 else 0.0\n",
    "\n",
    "    row = Row(\n",
    "        order_item_id   = f\"OI{100000+i}\",\n",
    "        order_id        = oid,\n",
    "        product_id      = pid,\n",
    "        quantity        = str(qty) if random.random()<P_STRING_NUMERIC_IN_FIELDS else f\"{qty}\",\n",
    "        unit_price      = str(unit) if random.random()<P_STRING_NUMERIC_IN_FIELDS else f\"{unit}\",\n",
    "        discount_amount = None if random.random()<P_NULLS_DISCOUNT_PROMO else (str(disc) if random.random()<0.5 else f\"{disc}\"),\n",
    "        promotion_id    = None if random.random()<P_NULLS_DISCOUNT_PROMO else f\"PR{random.randint(1,999):04d}\",\n",
    "        updated_at      = random_date_mixed_formats(random_date_between(365))\n",
    "    )\n",
    "    rows.append(row)\n",
    "\n",
    "# duplicar (order_id, product_id) com updated_at diferente (para dedupe por rn)\n",
    "for _ in range(int(N_ORDER_ITEMS * P_DUP_ORDERITEM_SAME_OP)):\n",
    "    base = random.choice(rows).asDict()\n",
    "    base[\"updated_at\"] = random_date_mixed_formats(random_date_between(365))\n",
    "    rows.append(Row(**base))\n",
    "\n",
    "df_items = spark.createDataFrame(rows, schema=items_schema)\n",
    "df_items.write.mode(\"overwrite\").format(\"delta\").saveAsTable(fqtn(\"order_items\"))\n",
    "display(df_items.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d84cb37-30fc-48c9-a508-4091801fec66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6) Checks rápidos (compatibilidade com suas consultas Silver/Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b619b7-9f94-430a-9e47-e6bd16d17fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Amostras e contagens\n",
    "print(\"orders:\",      spark.table(fqtn(\"orders\")).count())\n",
    "print(\"order_items:\", spark.table(fqtn(\"order_items\")).count())\n",
    "print(\"customers:\",   spark.table(fqtn(\"customers\")).count())\n",
    "print(\"products:\",    spark.table(fqtn(\"products\")).count())\n",
    "\n",
    "# Algumas verificações que seu material usa na Silver:\n",
    "spark.sql(f\"\"\"\n",
    "SELECT order_status, COUNT(*) c\n",
    "FROM {fqtn(\"orders\")}\n",
    "GROUP BY order_status\n",
    "ORDER BY c DESC\n",
    "\"\"\").show(10, False)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT order_id, COUNT(*) c\n",
    "FROM {fqtn(\"orders\")}\n",
    "GROUP BY order_id\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY c DESC\n",
    "\"\"\").show(5, False)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT order_id, product_id, COUNT(*) c\n",
    "FROM {fqtn(\"order_items\")}\n",
    "GROUP BY order_id, product_id\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY c DESC\n",
    "\"\"\").show(5, False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
